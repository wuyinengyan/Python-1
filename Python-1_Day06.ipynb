{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P55 054：spider1(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "******************************\n",
      "200\n",
      "Date: Mon, 14 Oct 2019 01:54:06 GMT\n",
      "Content-Type: image/jpeg\n",
      "Transfer-Encoding: chunked\n",
      "Connection: close\n",
      "Set-Cookie: __cfduid=d29686bf07638b85a51e8d715a42a931c1571018046; expires=Tue, 13-Oct-20 01:54:06 GMT; path=/; domain=.placekitten.com; HttpOnly\n",
      "Access-Control-Allow-Origin: *\n",
      "Cache-Control: public, max-age=86400\n",
      "Expires: Tue, 15 Oct 2019 01:54:06 GMT\n",
      "CF-Cache-Status: HIT\n",
      "Age: 78651\n",
      "Vary: Accept-Encoding\n",
      "Server: cloudflare\n",
      "CF-RAY: 5255ec672989c771-AMS\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# req = urllib.request.Request(\"http://placekitten.com/g/480/360\")\n",
    "# response = urllib.request.urlopen(req)\n",
    "# req.add_header(\"User-Agent\",\"\")  # 添加header的方法之二，动态追加header\n",
    "response = urllib.request.urlopen(\"http://placekitten.com/g/480/360\")  # 这是上两句的缩写\n",
    "cat_img = response.read()\n",
    "\n",
    "print(type(cat_img))\n",
    "print(\"*\"*30)\n",
    "print(response.getcode())  # Status Code：200 OK\n",
    "# with open(\"cat_01.jpg\", \"wb\") as f:  # 存储图片流为文件\n",
    "#     f.write(cat_img)\n",
    "print(response.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          {\"type\":\"EN2ZH_CN\",\"errorCode\":0,\"elapsedTime\":2,\"translateResult\":[[{\"src\":\"How could the world go back to the way it was when so much bad had happened?\",\"tgt\":\"这个世界怎么可以回到以前那么多不好的事情发生?\"}]]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用postman工具、并用PyCharm美化格式\n",
    "import requests\n",
    "\n",
    "# url = \"http://fanyi.youdao.com/translate_o?smartresult=dict&smartresult=rule\"  # 把_o删除掉\n",
    "url = \"http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule\"\n",
    "querystring = {\"i\": \"How could the world go back to the way it was when so much bad had happened?\",\n",
    "               \"from\": \"AUTO\", \"to\": \"AUTO\", \"smartresult\": \"dict\", \"client\": \"fanyideskweb\",\n",
    "               \"salt\": \"15709414299940\", \"sign\": \"b2932c3b29dc9542443efe2a537a5b6c\",\n",
    "               \"ts\": \"1570941429994\", \"bv\": \"e2a78ed30c66e16a857c5b6486a1d326\",\n",
    "               \"doctype\": \"json\", \"version\": \"2.1\", \"keyfrom\": \"fanyi.web\", \"action\": \"FY_BY_CLICKBUTTION\"}\n",
    "headers = {'cache-control': \"no-cache\",\n",
    "           'Postman-Token': \"a7f2e62a-8fb9-4838-acb6-0456e228b070\"}  #  User-Agent：添加header的方法之一，常用的方法\n",
    "response = requests.request(\"POST\", url, headers=headers, params=querystring)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入内容：How could the world go back to the way it was when so much bad had happened?\n",
      "翻译结果为：这个世界怎么可以回到以前那么多不好的事情发生?\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "content = input(\"请输入内容：\")\n",
    "url = \"http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule\"\n",
    "data = {\"i\": content, \"from\": \"AUTO\", \"to\": \"AUTO\", \"smartresult\": \"dict\", \"client\": \"fanyideskweb\",\n",
    "        \"salt\": \"15709414299940\", \"sign\": \"b2932c3b29dc9542443efe2a537a5b6c\",\n",
    "        \"ts\": \"1570941429994\", \"bv\": \"e2a78ed30c66e16a857c5b6486a1d326\",\n",
    "        \"doctype\": \"json\", \"version\": \"2.1\", \"keyfrom\": \"fanyi.web\", \"action\": \"FY_BY_CLICKBUTTION\"}\n",
    "data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "response = urllib.request.urlopen(url, data)\n",
    "html = response.read().decode(\"utf-8\")\n",
    "print(\"翻译结果为：%s\" % json.loads(html)['translateResult'][0][0]['tgt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P56 055：spider2(延迟、代理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入内容(输入'q!'退出程序)：How could the world go back to the way it was when so much bad had happened?\n",
      "翻译结果为：这个世界怎么可以回到以前那么多不好的事情发生?\n",
      "2019-10-14 10:08:29.020934\n",
      "2019-10-14 10:08:34.028543\n",
      "请输入内容(输入'q!'退出程序)：q!\n"
     ]
    }
   ],
   "source": [
    "# 延迟提交时间\n",
    "import urllib.request\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "while True:\n",
    "    content = input(\"请输入内容(输入'q!'退出程序)：\")\n",
    "    if content == \"q!\":\n",
    "        break\n",
    "    url = \"http://fanyi.youdao.com/translate?smartresult=dict&smartresult=rule\"\n",
    "    data = {\"i\": content, \"from\": \"AUTO\", \"to\": \"AUTO\", \"smartresult\": \"dict\", \"client\": \"fanyideskweb\",\n",
    "            \"salt\": \"15709414299940\", \"sign\": \"b2932c3b29dc9542443efe2a537a5b6c\",\n",
    "            \"ts\": \"1570941429994\", \"bv\": \"e2a78ed30c66e16a857c5b6486a1d326\",\n",
    "            \"doctype\": \"json\", \"version\": \"2.1\", \"keyfrom\": \"fanyi.web\", \"action\": \"FY_BY_CLICKBUTTION\"}\n",
    "    data = urllib.parse.urlencode(data).encode(\"utf-8\")\n",
    "    response = urllib.request.urlopen(url, data)\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "    print(\"翻译结果为：%s\" % json.loads(html)['translateResult'][0][0]['tgt'])\n",
    "    print(datetime.now())\n",
    "    time.sleep(5)  # 延迟5秒\n",
    "    print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 代理\n",
    "import urllib.request\n",
    "import random\n",
    "\n",
    "# url = \"https://www.whatisyourip.com/\"  # 由于目标计算机积极拒绝，无法连接\n",
    "url = \"http://www.bilibili.com/\"  # 可以成功\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\"}\n",
    "iplist = [\"101.4.136.34:81\", \"223.111.131.100:8080\", \"47.94.234.50:8080\"]\n",
    "\n",
    "proxy_support = urllib.request.ProxyHandler({\"http\": random.choice(iplist)})  # 参数是一个字典{\"类型\":\"代理ip:端口号\"}\n",
    "opener = urllib.request.build_opener(proxy_support)  # 2.定制创建一个opener\n",
    "opener.addheaders = [(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")]  # 方法一：加上这个，否则HTTPError: HTTP Error 403: Forbidden\n",
    "urllib.request.install_opener(opener)  # 3. 安装opener\n",
    "# 也可以使用opener.open(url)\n",
    "# req = urllib.request.Request(url, headers = headers)  # 方法二：加上这个，否则HTTPError: HTTP Error 403: Forbidden\n",
    "req = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(req)\n",
    "html = response.read().decode(\"utf-8\")\n",
    "with open(\"d:\\\\GitHub\\\\bilibili.txt\", \"w\", encoding='utf-8') as f:  # 注意加上encoding='utf-8'，否则各种UnicodeEncodeError:\n",
    "    f.write(html)\n",
    "# print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P57 056：spider3(实战)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://sc.chinaz.com/tupian/meinvtupian.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_2.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_3.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_4.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_5.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_6.html\n",
      "http://sc.chinaz.com/tupian/meinvtupian_7.html\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(40690 bytes read, 112622 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-01f50e37e8d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[0mdownload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-01f50e37e8d2>\u001b[0m in \u001b[0;36mdownload_img\u001b[1;34m(folder, page_start, page_end)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msub_url\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msub_url_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mimg_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_url\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 获取网页中图片的地址\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msave_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 保存图片到指定的文件夹\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfind_sub_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-01f50e37e8d2>\u001b[0m in \u001b[0;36msave_img\u001b[1;34m(img_url)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_url\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# 分割后的数组，取最后一个为文件名称\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-01f50e37e8d2>\u001b[0m in \u001b[0;36murl_open\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    458\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mamt\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(40690 bytes read, 112622 more expected)"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_img(folder=\"SerectGuard\", page_start=0, page_end=7):\n",
    "    os.mkdir(folder)  # 同linux命令：创建文件夹\n",
    "    os.chdir(folder)  # 工作目录，文件保存至本文件夹\n",
    "    url = \"http://sc.chinaz.com/tupian/\"  # 可以成功\n",
    "    \n",
    "    sub_url_list = []\n",
    "    for i in range(page_start, page_end):\n",
    "        str_page_num = \"meinvtupian_\" + str(i+1) if i > 0 else \"meinvtupian\"\n",
    "        page_url = \"%s.html\" % (url + str_page_num)\n",
    "        sub_url_list.extend(find_sub_urls(page_url))  # 获取网页中的下一级图片页面网站列表\n",
    "    \n",
    "    for sub_url in sub_url_list:\n",
    "        img_url = find_img(sub_url)  # 获取网页中图片的地址\n",
    "        save_img(img_url)  # 保存图片到指定的文件夹\n",
    "\n",
    "def find_sub_urls(url):\n",
    "    print(url)\n",
    "    try:  # 应对IncompleteRead: \n",
    "        html = url_open(url).decode(\"utf-8\")  \n",
    "    except Exception as e:\n",
    "        content = e.partial\n",
    "        html = content.decode(\"utf-8\")\n",
    "        if len(html) == 0:\n",
    "            html = \"{}\"\n",
    "        \n",
    "    soup = BeautifulSoup(html, \"lxml\") \n",
    "    li = soup.find_all(class_=\"box picblock col3\")\n",
    "    sub_url_list = []\n",
    "    for item in li:\n",
    "        soup1 = BeautifulSoup(str(item), \"lxml\")\n",
    "        sub_url_list.append(soup1.select(\"div div a\")[0].get(\"href\"))\n",
    "    return sub_url_list\n",
    "# def find_sub_urls(url):\n",
    "#     html = url_open(url).decode(\"utf-8\")\n",
    "#     sub_url_list = []\n",
    "    \n",
    "#     a = html.find(\"img src2=\")  \n",
    "#     while a != -1:\n",
    "#         b = html.find(\"_s.jpg\", a, a+255)  # 从a的位置开始找，最多不超过255位\n",
    "#         if b != -1:\n",
    "#             sub_url = html[a+10:b+6]  # 去除10个字符——img src2=\"，加上结尾的6个字符——_s.jpg\n",
    "#             if sub_url not in sub_url_list:  # 如果不存在，则保存如列表\n",
    "#                 sub_url_list.append(sub_url)\n",
    "#         else:  # 如果超过255个字符，依然没有找到b，则跳过10个字符——img src2=\"，重新开始找\n",
    "#             b = a + 10\n",
    "#         a = html.find(\"img src2=\", b)  # 从上一次的结尾处，重新开始寻找\n",
    "#     return sub_url_list\n",
    "        \n",
    "def find_img(url):\n",
    "    try:\n",
    "        html = url_open(url).decode(\"utf-8\")  \n",
    "    except Exception as e:\n",
    "        content = e.partial\n",
    "        html = content.decode(\"utf-8\")\n",
    "        if len(html) == 0:\n",
    "            html = \"{}\"\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(html, \"lxml\") \n",
    "    return soup.find(class_=\"imga\").select(\"a img\")[0].get(\"src\")\n",
    "\n",
    "\n",
    "# 保存图片\n",
    "def save_img(img_url):\n",
    "    filename = img_url.split(\"/\")[-1]  # 分割后的数组，取最后一个为文件名称\n",
    "    with open(filename, 'wb') as f:\n",
    "        img = url_open(img_url)\n",
    "        f.write(img)\n",
    "        \n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")\n",
    "    \n",
    "    # 使用代理速度变慢，且会有服务器的公告\n",
    "    iplist = [\"101.4.136.34:81\", \"223.111.131.100:8080\", \"47.94.234.50:8080\"]\n",
    "    proxy_support = urllib.request.ProxyHandler({\"http\": random.choice(iplist)})  # 参数是一个字典{\"类型\":\"代理ip:端口号\"}\n",
    "    opener = urllib.request.build_opener(proxy_support)  # 2.定制创建一个opener\n",
    "    urllib.request.install_opener(opener)  # 3. 安装opener\n",
    "    \n",
    "    response = urllib.request.urlopen(url)\n",
    "    return response.read()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
